{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINNS for inverse problems (heat equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8598,
     "status": "ok",
     "timestamp": 1732815970300,
     "user": {
      "displayName": "LUCA PELLEGRINI",
      "userId": "06842304188432351891"
     },
     "user_tz": -60
    },
    "id": "OjjGUt3jg9Ff"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from DNN import DeepNet\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.manual_seed(128)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define an helper function for converting data from torch tensors in the GPU to numpy arrays in the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "  return x.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat equation + DNN\n",
    "Consider the one-dimensional heat equation:\n",
    "$$\n",
    "u_t(t, x) = k(t,x)u_{xx}(t, x) + s(t, x), \\quad t\\in[0,T], ~x\\in [-1,0]\n",
    "$$\n",
    "with zero Dirichlet boundary conditions\n",
    "$$\n",
    "u_b(t, -1)=u_b(t,0)=0,\n",
    "$$\n",
    "and initial condition\n",
    "$$\n",
    "u(0, x) = u_0(x) = - \\sin(\\pi x)\n",
    "$$\n",
    "The equation parameter $k:[0,T]\\times[-1,0]\\mapsto \\mathbb{R} $ denotes the medium conductivity and $s:[0,T]\\times[-1,0]\\mapsto \\mathbb{R}$ is a source term. Suppose that $s(t,x)$ is given and we want to estimate $k(t,x)$ and approximate $u(t,x)$ with two different neural networks.\n",
    "\n",
    "We will use a physics-informed neural network (PINN) to solve this inverse problem with tunable parameters $\\theta$ and $\\phi$:\n",
    "$$\n",
    "u_\\theta(t,x) \\approx u(t,x), \\quad k_\\phi(t, x) \\approx k(t, x).\n",
    "$$\n",
    "Initialize $u_{\\theta}$ as a neural network with $2$ input, $1$ output and $5$ hidden layers with $40$ neurons each. Initialize $k_{\\phi}(t,x) = k_{\\phi}(x)$ as a neural network with $1$ input, $1$ output and $5$ hidden layers with $40$ neurons each, both the networks use the hyperbolic tangent as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_input = \n",
    "sol_output = \n",
    "sol_hidden_layer =\n",
    "\n",
    "coef_input = \n",
    "coef_output = \n",
    "coef_hidden_layer =\n",
    "activation_function =\n",
    "\n",
    "approx_sol = DeepNet()\n",
    "approx_coef = DeepNet()\n",
    "print(approx_sol,approx_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "We want to solve a parameter identification inverse problem, which can be formulated as follows:\n",
    "\n",
    "Given observation measurements of the medium temperature\n",
    "\n",
    "$$\n",
    "S_{meas}=\\{(t_i, x_i, u^{meas}_{i})\\}_{i=1}^{N_{meas}},\n",
    "$$\n",
    "find $u: [0,T]\\times[-1,0]\\mapsto\\mathbb{R}$ and $k:[-1,0]\\mapsto \\mathbb{R}$, such that\n",
    "\n",
    "$$u_t(t, x) = k(t,x)u_{xx}(t, x) + s(t,x),\n",
    "\\\\ u_b(t, -1)=u_b(t,0)=0 \\quad\\text{   and  } \\quad u(x, 0) = u_0(x) = -\\sin(\\pi x).$$\n",
    "\n",
    "Here the code to create the necessary datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1732816441855,
     "user": {
      "displayName": "LUCA PELLEGRINI",
      "userId": "06842304188432351891"
     },
     "user_tz": -60
    },
    "id": "SKJBTfMxn2aA",
    "outputId": "9e3dd6a2-75e8-41cb-fe2e-466a9583890b"
   },
   "outputs": [],
   "source": [
    "x_ini, x_fin =\n",
    "t_ini = \n",
    "t_fin = 0.1  \n",
    "\n",
    "N_int = 100  # number of collocation points\n",
    "N_0 = 150  # collocation of the initial data\n",
    "N_b = 50  # collocation of the boundary condition\n",
    "N_data = 20 # number of data points\n",
    "\n",
    "x_int = np.random.uniform(x_ini, x_fin, (N_int, 1))\n",
    "t_int = np.random.uniform(t_ini, t_fin, (N_int, 1))\n",
    "\n",
    "x_data = torch.linspace(x_ini,x_fin,N_data)\n",
    "t_data = torch.linspace(t_ini,t_fin,N_data)\n",
    "\n",
    "data = torch.cartesian_prod(t_data,x_data)\n",
    "\n",
    "x_0 = np.random.uniform(x_ini, x_fin, (N_0, 1))\n",
    "t_0 = np.zeros((N_0, 1))\n",
    "\n",
    "\n",
    "t_b = np.random.uniform(t_ini, t_fin, (N_b, 1))\n",
    "x_b1 = np.zeros((N_b, 1)) + x_ini\n",
    "x_b2 = np.zeros((N_b, 1)) + x_fin\n",
    "# DATA plot\n",
    "p1 = plt.figure(1)\n",
    "plt.scatter(x_int, t_int, marker=\"x\",label ='Collocation')\n",
    "plt.scatter(x_0, t_0, marker=\".\", c=\"k\",label = 'Initial data')\n",
    "plt.scatter(x_b1, t_b, marker=\"d\", facecolors=\"none\", edgecolors=\"r\",label = 'Boundary')\n",
    "plt.scatter(x_b2, t_b, marker=\"d\", facecolors=\"none\", edgecolors=\"r\")\n",
    "plt.scatter(data[:,1].cpu(),data[:,0].cpu(),marker = '.',label = 'Data')\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1732816444656,
     "user": {
      "displayName": "LUCA PELLEGRINI",
      "userId": "06842304188432351891"
     },
     "user_tz": -60
    },
    "id": "JVR0zVaLtyUt"
   },
   "outputs": [],
   "source": [
    "# reshape of the data\n",
    "x_int = torch.Tensor(x_int).to(device).requires_grad_(True).view(-1, 1)\n",
    "t_int = torch.Tensor(t_int).to(device).requires_grad_(True).view(-1, 1)\n",
    "\n",
    "x_0 = torch.Tensor(x_0).to(device).requires_grad_(False).view(-1, 1)\n",
    "t_0 = torch.Tensor(t_0).to(device).requires_grad_(False).view(-1, 1)\n",
    "\n",
    "x_data = torch.Tensor(x_data).to(device).requires_grad_(False).view(-1, 1)\n",
    "t_data = torch.Tensor(t_data).to(device).requires_grad_(False).view(-1, 1)\n",
    "\n",
    "\n",
    "t_b = torch.Tensor(t_b).to(device).requires_grad_(False).view(-1, 1)\n",
    "x_b0 = torch.Tensor(x_b1).to(device).requires_grad_(False).view(-1, 1)\n",
    "x_b1 = torch.Tensor(x_b2).to(device).requires_grad_(False).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial conditions and true solution\n",
    "Here we define the source term $s(t,x)$ and the initial condition $u_0(x)$. We also define the true conductivity $k(t,x)$ and the true solution $u(t,x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732815970301,
     "user": {
      "displayName": "LUCA PELLEGRINI",
      "userId": "06842304188432351891"
     },
     "user_tz": -60
    },
    "id": "eSwd3aORhOdS"
   },
   "outputs": [],
   "source": [
    "# Initial condition to solve the heat equation u0(x)=-sin(pi x)\n",
    "def initial_condition(x):\n",
    "    return \n",
    "\n",
    "# Exact solution for the heat equation ut = u_xx with the IC above\n",
    "def exact_sol(x,t):\n",
    "    return u\n",
    "\n",
    "def exact_conductivity(x):\n",
    "    return k\n",
    "\n",
    "def source(x,t):\n",
    "    return s\n",
    "\n",
    "u_exact_data = exact_sol(x_data,t_data)\n",
    "u_0 = initial_condition(x_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1732816000231,
     "user": {
      "displayName": "LUCA PELLEGRINI",
      "userId": "06842304188432351891"
     },
     "user_tz": -60
    },
    "id": "-CUmTjzew3mV"
   },
   "outputs": [],
   "source": [
    "def eval_on_xt(Phi, x, t):  # needed for the evaluation of the network on two points\n",
    "    return Phi(torch.cat([x, t], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDE loss\n",
    "We define the following residuals:\n",
    "\n",
    "   - Interior residual given by,\n",
    "   $$r_{int,\\theta, \\phi}(t, x):=  u_{\\theta}(t, x) - k_\\phi(t,x)u_{\\theta, xx}(t, x) -s(t,x), \\quad \\forall ~t \\in [0,T],~ x \\in [-1,0].$$\n",
    "   \n",
    "\n",
    "        \n",
    "   - Spatial boundary residual given by,\n",
    "   \n",
    "        $$r_{sb,\\theta}(t,-1):= u_{\\theta}(t,-1)- u_b(t,-1), \\quad r_{sb,\\theta}(t,0):= u_{\\theta}(t,0)- u_b(t,0), \\quad \\forall t \\in (0,T].$$\n",
    "        \n",
    "   - Temporal boundary residual given by,\n",
    "   \n",
    "        $$r_{tb,\\theta}(x):= u_{\\theta}(x,0) - u_0(x), \\quad \\forall x \\in [-1,0].$$\n",
    "\n",
    "and compute the corresponding loss functions:\n",
    "\n",
    "$$\n",
    "L_{int}(\\theta, \\phi) = \\int_{[0,T]\\times[-1,0]}r_{int,\\theta, \\phi}^2(t, x) dtdx, \\quad\n",
    "L_{sb}(\\theta) = \\int_{[0,T]}r_{sb,\\theta}^2(t,-1) dt + \\int_{[0,T]}r_{sb,\\theta}^2(t,0)dt, \\quad\n",
    "L_{tb}(\\theta) = \\int_{[-1,0]}r_{tb,\\theta}^2(x) dx\n",
    "$$\n",
    "\n",
    "The loss functions include integrals that can be approximated by suitable quadrature rule. We use quasi Monte-Carlo and accordingly define the following training sets\n",
    "\n",
    "$$\n",
    "S_{int} =\\{y_n\\}, \\quad 1 \\leq n \\leq N_{int},\\quad y_n = (x,t)_n \\in D_T,\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{sb, -1} =\\{t_n, u_b(t_n,-1) \\}, \\quad 1 \\leq n \\leq N_{sb}, t_n \\in [0,T],\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{sb, 0} =\\{t_n, u_b(t_n,0) \\}, \\quad 1 \\leq n \\leq N_{sb}, t_n \\in [0,T],\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{tb}=\\{x_n, u_0(x_n)\\}\\quad  1 \\leq n \\leq N_{tb}, x_n \\in [-1,0].\n",
    "$$\n",
    "\n",
    "\n",
    "So the loss functions can be approximated by\n",
    "$$\n",
    "L_{int}(\\theta, \\phi) = \\frac{1}{N_{int}}\\sum_{i=1}^{N_{int}}r_{int,\\theta,\\phi}^2(y_n), \\quad\n",
    "L_{sb}(\\theta) = \\frac{1}{N_{sb}}\\sum_{i=1}^{N_{sb}}r_{sb,\\theta}^2(t_n,-1) + \\frac{1}{N_{sb}}\\sum_{i=1}^{N_{sb}}r_{sb,\\theta}^2(t_n,0), \\quad\n",
    "L_{tb}(\\theta) = \\frac{1}{N_{tb}}\\sum_{i=1}^{N_{tb}}r_{tb,\\theta}^2(x_n)\n",
    "$$\n",
    "\n",
    "Also the measurements data $S_{meas}$ have to taken into account. To do so, we define ad additional loss term:\n",
    "$$\n",
    "L_{meas} = \\frac{1}{N_{meas}}\\sum_{i=1}^{N_{mean}}(u_\\theta(t_i, x_i) - u^{meas}_i)^2(x_n)\n",
    "$$\n",
    "and eventually solve the following minimization problem\n",
    "\n",
    "$$\n",
    "\\theta^\\ast, \\phi^\\ast = argmin_{\\theta, \\phi} \\Big(L_{int}(\\theta, \\phi) + \\lambda_u L_u(\\theta)\\Big)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "L_u(\\theta) = L_{tb}(\\theta) + L_{sb}(\\theta) + L_{meas}(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss_pde(Phi,Psi,x,t,metric):\n",
    "    x.requires_grad_(True)\n",
    "    t.requires_grad_(True)\n",
    "    \n",
    "    u = eval_on_xt(Phi,x,t)\n",
    "    k = Psi(x)\n",
    "    s = source(x,t)\n",
    "    u_x = torch.autograd.grad()[0]\n",
    "    u_xx = torch.autograd.grad()[0]\n",
    "    u_t = torch.autograd.grad()[0]\n",
    "\n",
    "    return metric()\n",
    "\n",
    "def eval_loss_pt(Phi,x,t,y):\n",
    "    return ().pow(2).mean()\n",
    "\n",
    "def eval_loss_data(Phi,x,t,data,metric):\n",
    "    return metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 6000\n",
    "optimizer =  optim.Adam(list(approx_sol.parameters())+list(approx_coef.parameters()),lr = 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,100,0.98)\n",
    "\n",
    "loss_values = np.empty([n_epochs+1,8]) # global,pde,t0,b0,b1,data,kb1,kb2\n",
    "STEP = 100\n",
    "MSE =  torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "  \n",
    "  loss_pde = eval_loss_pde()\n",
    "  \n",
    "  loss_t0 = eval_loss_pt()\n",
    "  loss_b0 = eval_loss_pt()\n",
    "  loss_b1 = eval_loss_pt()\n",
    "  loss_data = eval_loss_data()\n",
    "\n",
    "  loss_k_b1 = approx_coef(x_b0).pow(2).mean()\n",
    "  loss_k_b2 = approx_coef(x_b1).pow(2).mean()\n",
    "\n",
    "  loss = 1.5*loss_pde +1.5*loss_t0 + loss_b0 + loss_b1 + loss_data + loss_k_b1 + loss_k_b1 + loss_k_b2\n",
    "\n",
    "  #print(w_pde,w_t0,w_b1,w_b2,w_data)\n",
    "  # zero gradients\n",
    "  ...\n",
    "  # loss backward\n",
    "  ...\n",
    "  # optimizer step\n",
    "  ...\n",
    "  # scheduler step\n",
    "  ...\n",
    "\n",
    "  loss_values[epoch,:] = np.stack([convert(loss),convert(loss_pde),convert(loss_t0),convert(loss_b0),convert(loss_b1),convert(loss_data),convert(loss_k_b1),convert(loss_k_b2)])\n",
    "\n",
    "  if epoch%STEP == 0:\n",
    "\n",
    "    print(f'Epoch {epoch} | Global loss {loss_values[epoch,0]:.4e} | pde {loss_values[epoch,1]:.4e} | t0 {loss_values[epoch,2]:.4e} | b1 {loss_values[epoch,3]:.4e} | b2 {loss_values[epoch,4]:.4e} | data {loss_values[epoch,5]:.4e} | k boundaries {loss_values[epoch,6]:.4e} {loss_values[epoch,7]:.4e} ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize loss function during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = plt.figure(2, (15, 8))\n",
    "# global,pde,t0,b0,b1,data,kb1,kb2\n",
    "e = range(n_epochs + 1)\n",
    "ax1, ax2 = p2.subplots(1, 2)\n",
    "ax1.semilogy(e, loss_values[:, 0], linewidth=0.7)\n",
    "ax1.set_title(\"Global Loss\")\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.ticklabel_format(style=\"sci\", axis=\"x\", scilimits=(0, 0))\n",
    "ax1.grid()\n",
    "\n",
    "# plt.ylabel('Loss values')\n",
    "ax2.semilogy(e, loss_values[:, 1], linewidth=0.7, label=\"$Loss_f$\")\n",
    "ax2.semilogy(e, loss_values[:, 2], linewidth=0.7, label=\"$Loss_0$\")\n",
    "ax2.semilogy(e, loss_values[:, 3], linewidth=0.7, label=\"$Loss_{b0}$\")\n",
    "ax2.semilogy(e, loss_values[:, 4], linewidth=0.7, label=\"$Loss_{b1}$\")\n",
    "ax2.semilogy(e, loss_values[:, 5], linewidth=0.7, label=\"$Loss_{data}$\")\n",
    "ax2.semilogy(e, loss_values[:, 6], linewidth=0.7, label=\"$Loss_{kb1}$\")\n",
    "ax2.semilogy(e, loss_values[:, 7], linewidth=0.7, label=\"$Loss_{kb2}$\")\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_title(\"Components\")\n",
    "ax2.ticklabel_format(style=\"sci\", axis=\"x\", scilimits=(0, 0))\n",
    "ax2.grid()\n",
    "p2.legend(loc=\"lower right\", ncol=7)\n",
    "\n",
    "plt.suptitle(\"Values of the Loss function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the true and predicted solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1732816821449,
     "user": {
      "displayName": "LUCA PELLEGRINI",
      "userId": "06842304188432351891"
     },
     "user_tz": -60
    },
    "id": "4EAwWQtz7j7p"
   },
   "outputs": [],
   "source": [
    "N_int_tst = 300\n",
    "N_0_tst = 1000  # same sample for initial data and initial derivate\n",
    "N_b_tst = 1000\n",
    "\n",
    "x_test = torch.linspace(x_ini, x_fin, N_int_tst)\n",
    "t_test = torch.linspace(t_ini, t_fin, N_int_tst)\n",
    "\n",
    "x_grid, t_grid = torch.meshgrid(x_test, t_test, indexing=\"ij\")\n",
    "XT = torch.stack([x_grid.ravel(), t_grid.ravel()], axis=-1)\n",
    "\n",
    "x_0_tst = torch.linspace(x_ini, x_fin, N_0_tst).reshape(1, -1).t()\n",
    "t_0_tst = torch.zeros_like(x_0_tst)\n",
    "t_0t_tst = t_0_tst.requires_grad_(True)  # needed for the evaluation of the derivate\n",
    "\n",
    "t_b_tst = torch.linspace(t_ini, t_fin, N_b_tst).reshape(1, -1).t()\n",
    "x_b1_tst = torch.zeros_like(t_b_tst)\n",
    "x_b2_tst = torch.zeros_like(t_b_tst) + x_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1732816899871,
     "user": {
      "displayName": "LUCA PELLEGRINI",
      "userId": "06842304188432351891"
     },
     "user_tz": -60
    },
    "id": "RVl6WgLh4VHZ"
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    # Get the model's prediction\n",
    "    U_theta = convert(eval_on_xt(approx_sol, XT[:, 0:1], XT[:, 1:2]))\n",
    "    # Reshape U_theta to have the same structure as your grid\n",
    "    U_theta = U_theta.reshape(x_grid.shape)\n",
    "    u_ex_tst = convert(exact_sol(x_grid, t_grid))\n",
    "    x_grid = convert(x_grid)\n",
    "    t_grid = convert(t_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "executionInfo": {
     "elapsed": 1725,
     "status": "ok",
     "timestamp": 1732816931109,
     "user": {
      "displayName": "LUCA PELLEGRINI",
      "userId": "06842304188432351891"
     },
     "user_tz": -60
    },
    "id": "EeCKyCz676Jn",
    "outputId": "139a49d4-324e-4755-abe8-f6aa8757836d"
   },
   "outputs": [],
   "source": [
    "# Plot the surface\n",
    "p3 = plt.figure(3, figsize=plt.figaspect(0.5))\n",
    "\n",
    "ax = p3.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "ax.plot_surface(x_grid, t_grid, u_ex_tst, cmap=\"viridis\")\n",
    "ax.set_title(\"Exact solution\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"t\")\n",
    "ax = p3.add_subplot(1, 2, 2, projection=\"3d\")\n",
    "ax.plot_surface(x_grid, t_grid, U_theta, cmap=\"viridis\")\n",
    "ax.set_title(r\"$u_{\\Theta}$\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"t\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(convert(x_int),convert(exact_conductivity(x_int)))\n",
    "plt.scatter(convert(x_int),convert(approx_coef(x_int)))\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMmn+BbkfOoH2BQulbvM1wQ",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
